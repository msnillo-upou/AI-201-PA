{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac844f4-b53a-4043-8151-cf0fe3ceb66e",
   "metadata": {},
   "source": [
    "# AI201 Programming Assignment 2\n",
    "## Naive Bayes Spam Filter\n",
    "\n",
    "*Submitted by: Mike Allan Nillo*\n",
    "\n",
    "### Table of Contents\n",
    "- Loading of Data\n",
    "- Classifier Construction and Evaluation\n",
    "- Lambda Smoothing\n",
    "- Improving your Classifier\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478a8ab-443f-41e9-a8a9-1c65cabaa3e6",
   "metadata": {},
   "source": [
    "### Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d6b9408-c771-462e-bf6b-435deb4543e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Step 1: Load the Labels\n",
    "labels = {}\n",
    "with open('/home/mikeallan/analytics/meng-ai/ai-201/AI201_PA2_Spam_Filter_2SAY23-24/AI201_PA2_Spam_Filter_2SAY23-24/trec06p-ai201/labels', 'r') as f:\n",
    "    for line in f:\n",
    "        label, rel_path = line.strip().split(' ')\n",
    "        filename = os.path.basename(rel_path)\n",
    "        labels[filename] = label\n",
    "\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "data = []\n",
    "base_path = '/home/mikeallan/analytics/meng-ai/ai-201/AI201_PA2_Spam_Filter_2SAY23-24/AI201_PA2_Spam_Filter_2SAY23-24/trec06p-ai201/data'\n",
    "\n",
    "# Walk through each directory and file in the base_path\n",
    "for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirpath, filename)\n",
    "        with open(file_path, 'r', errors='ignore') as f:\n",
    "            # Read the file content\n",
    "            content = f.read()\n",
    "            # Get the label from the labels dictionary\n",
    "            label = labels.get(filename, 'unknown')\n",
    "            data.append((content, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f636fe-a00e-4d28-811c-c6f19d428740",
   "metadata": {},
   "source": [
    "### Classifier Construction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b1e72a-f30f-4bbf-ab32-dd89f98e4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Assume 'data' is a list of tuples, where each tuple contains the file content and the label\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the index that separates the training data from the test data\n",
    "split_index = int(len(data) * 0.7)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "# Separate the texts and the labels\n",
    "texts_train, labels_train = zip(*train_data)\n",
    "texts_test, labels_test = zip(*test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fff278b-5cbd-4d33-b7eb-6e94c09ef537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability for spam: 0.7821045475147304\n",
      "Prior probability for ham: 0.21789545248526968\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries for spam and ham words\n",
    "spam_words = {}\n",
    "ham_words = {}\n",
    "\n",
    "# Initialize counters for spam and ham documents\n",
    "spam_docs = 0\n",
    "ham_docs = 0\n",
    "\n",
    "# Parse the documents in the training set\n",
    "for text, label in zip(texts_train, labels_train):\n",
    "    # Convert the text to lower case and replace commas and periods with spaces\n",
    "    text = text.lower().replace(',', ' ').replace('.', ' ')\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Update the appropriate dictionaries\n",
    "    if label == 'spam':\n",
    "        spam_docs += 1\n",
    "        for word in words:\n",
    "            if word.isalpha():  # Check if the word contains only alphabetic characters\n",
    "                spam_words[word] = spam_words.get(word, 0) + 1\n",
    "    else:\n",
    "        ham_docs += 1\n",
    "        for word in words:\n",
    "            if word.isalpha():  # Check if the word contains only alphabetic characters\n",
    "                ham_words[word] = ham_words.get(word, 0) + 1\n",
    "\n",
    "# Form the vocabulary of unique words in the training data\n",
    "vocabulary = set(spam_words.keys()).union(set(ham_words.keys()))\n",
    "\n",
    "# Count the total number of documents\n",
    "total_docs = spam_docs + ham_docs\n",
    "\n",
    "# Calculate and report the prior probabilities for spam and ham\n",
    "prior_spam = spam_docs / total_docs\n",
    "prior_ham = ham_docs / total_docs\n",
    "\n",
    "print(f'Prior probability for spam: {prior_spam}')\n",
    "print(f'Prior probability for ham: {prior_ham}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97963f74-50e5-443a-add3-9c3f4b6d3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of words in spam and ham documents\n",
    "total_spam_words = sum(spam_words.values())\n",
    "total_ham_words = sum(ham_words.values())\n",
    "\n",
    "# Calculate the total number of words in the vocabulary\n",
    "total_words = len(vocabulary)\n",
    "\n",
    "# Initialize dictionaries for spam and ham probabilities\n",
    "spam_probs = {}\n",
    "ham_probs = {}\n",
    "\n",
    "# Calculate the word probabilities for spam and ham\n",
    "for word in vocabulary:\n",
    "    spam_probs[word] = (spam_words.get(word, 0) + 1) / (total_spam_words + total_words)\n",
    "    ham_probs[word] = (ham_words.get(word, 0) + 1) / (total_ham_words + total_words)\n",
    "\n",
    "# Define a function to classify a text as spam or ham\n",
    "def classify(text):\n",
    "    # Convert the text to lower case and replace commas and periods with spaces\n",
    "    text = text.lower().replace(',', ' ').replace('.', ' ')\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Initialize the spam and ham probabilities with the prior probabilities\n",
    "    spam_prob = prior_spam\n",
    "    ham_prob = prior_ham\n",
    "    # Update the probabilities for each word in the text\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            spam_prob *= spam_probs[word]\n",
    "            ham_prob *= ham_probs[word]\n",
    "    # Return the class with the highest probability\n",
    "    return 'spam' if spam_prob > ham_prob else 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b174463-9da5-4209-82f9-81b6a03eb49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4299814929056138\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for correct and total predictions\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Classify the documents in the test set\n",
    "for text, true_label in zip(texts_test, labels_test):\n",
    "    # Classify the text\n",
    "    predicted_label = classify(text)\n",
    "    # Update the counters\n",
    "    if predicted_label == true_label:\n",
    "        correct_predictions += 1\n",
    "    total_predictions += 1\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fab3ee4e-95d4-4d60-a74e-0657e271c622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7907692307692308\n",
      "Recall: 0.37434173669467785\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision_recall(predictions, labels):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    tp = fp = fn = 0\n",
    "\n",
    "    # Count the true positives, false positives, and false negatives\n",
    "    for predicted, true in zip(predictions, labels):\n",
    "        if predicted == 'spam':\n",
    "            if true == 'spam':\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        elif true == 'spam':\n",
    "            fn += 1\n",
    "\n",
    "    # Calculate and return the precision and recall\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "# Use the function to calculate the precision and recall\n",
    "predictions = [classify(text) for text in texts_test]\n",
    "precision, recall = calculate_precision_recall(predictions, labels_test)\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57e457-915c-4fdd-b594-df4990e6b2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
